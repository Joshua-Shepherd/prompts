* Prompts
This is a free and open-source (FOSS) curation
of prompts for OpenAI's GPT-3, EleutherAI's
GPT-j, and future LMs.

| License |
|---------|
| *GPL-3* |

** Prompts are regularly reviewed and tested under any branch
Push your own branch. Request access to the =semiosis= organisation.

*** Test results go here
https://github.com/semiosis/prompt-tests/

** The =.prompt= file format
This is the format I have used to organise
these prompts. It is =yaml= with a schema,
which is rapidly growing.

This is a very basic =.prompt= file that should work.

#+BEGIN_SRC yaml -n :async :results verbatim code
  title: "My new prompt"
  # Make sure to increment the prompt-version every time you update
  # so they will be tested.
  prompt-version: 1
  prompt: "Complete this sentence"
  temperature: 0.8
  max-tokens: 60
#+END_SRC

This =YASnippet= snippet contains an explanation of the =.prompt= file format.

#+BEGIN_SRC yaml -n :async :results verbatim code
  # -*- mode: snippet -*-
  # name: prompt
  # group: prompt-engineering
  # key: pr
  # expand-env: ((yas-indent-line 'fixed))
  # --
  # --------
  # Doc keys
  # --------

  # A TODO list.
  todo:

  # A list of design patterns used.
  design-patterns:

  # Possible other names for this prompt.
  future-titles: ""

  # Aims for developing this prompt.
  aims: |+
  - More abstractive rewording

  # Function documentation.
  doc: "Given ... ${1:title}"

  # A list of problems with the prompt.
  problems:
  - "Struggles with the latter columns."

  # ---------------
  # Functional keys
  # ---------------

  # A prompt which is in development will not be loaded by pen.el
  in-development: yes

  # An indicator that this prompt needs further work.
  needs-work: no

  # A title for the prompt. This will become the function name.
  title: "${1:title}"

  # Increment this number every time you make a functional change.
  # The test suite will only rerun if this version is incremented.
  prompt-version: 1

  # <:pp> defines a point where the following
  # text is concatenated before the postprocessor
  # is run.
  # <1>, <2> etc. are where variables are substituted
  # <1> is special because it may be the current selection
  # <2> May be inferred from <1> via a prompt.
  # This way, a function can be curried/beta-reduced to a function of 1 argument.
  prompt: |+
      ${2:contents}
    
      <1> are like <2> in that

  # Additional transformation of prompt after the template
  prompt-filter: "sed -z 's/\\s\\+$//'"

  # These are elisp String->String functions and run from pen.el
  # It probably runs earlier than the preprocessors shell scripts
  pen-preprocessors:
  - "pen-pf-correct-grammar"

  # 
  gpt-generator: "lm-complete"

  # myrc will select the completion engine using my config.
  # This may be openi-complete or something else
  engine: "myrc"

  # if nothing is selected in myrc and openapi-complete is used
  # by default, then openai should select this engine.
  preferred-openai-engine: "davinci"

  # 0.0 = /r/hadastroke
  # 1.0 = /r/iamveryrandom
  # Use 0.3-0.8
  temperature: 0.8

  # This is the max tokens requested from the LM
  max-tokens: 60

  top-p: 1.0

  # Not available yet: openai api completions.create --help
  frequency-penalty: 0.5

  # If I make presence-penalty 0 then it will get very terse
  presence-penalty: 0.0

  best-of: 1

  # Remove whitespace from the beginning of the response string
  chomp-start: on

  # Remove whitespace from the end of the response string
  chomp-end: off

  # Currently the OpenAI API can only accept one stop-sequence.
  # So only the first one will be used by the API,
  # but the completer script can make use the others.
  stop-sequences:
  - "\\n"
  - "\\n\\n"
  - "##"

  # Cache the function by default when running the prompt function
  cache: on

  # Names for the variables of the prompt function.
  # The first variable may be captured by selection, rather than manually entered.
  vars:
  - "former"
  - "latter"

  # Examples of typical values for the variables
  examples:
  - "boysenberries"
  - "strawberries"

  # A preprocessor may be run on the variable inputs before entering the prompt
  preprocessors:
  - "sed 's/^/- /"
  - "cat"

  # Completion indicates that this prompt can be used as a company-mode completion function.
  # Either ensure only one variable, or provide pen-defaults (functions that obtain the value automatically).
  # Otherwise, it will not work as expected.
  completion: on

  # Enable running conversation. This would be used for chatbots and REPLs.
  conversation-mode: no

  # ------------------------------------
  # Non-functional (in development) keys
  # ------------------------------------

  # These are expressions run from within Pen to give the value for the variable
  pen-defaults:
  - "(detect-language)"
  - "(pen-preceding-text)"

  # Output to test against. Possibly using similarity.
  test-output: "both are types of berry"

  # This compares the output of the external script to the output of the LM
  similarity-test: "compare <1> <2>"

  # Prefer the external command if it's available.
  prefer-external: on

  # This is an optional external command which may be used to perform the same task as your prompt.
  # This could be used in future to train the prompt.
  # The external command must take all variables as arguments (no stdin).
  # echo would simply result in the prompt function returning all the arguments as given.
  external: "echo"

  # This script returns a 0-1 decimal value representing the quality of the generated output.
  # The input is 2 arguments each containing output
  # The output is a decimal number from 0 to 1
  quality-script: "my-quality-checker-for-this-prompt.sh"

  # This script can be used to validate the output.
  # If the output is accurate, the validation script returns exit code 1.
  # The input is 2 arguments each containing output
  validation-script: "my-validator-for-this-prompt.sh"

  # This is the name of an external database-driven pretext generator.
  # It would typically summarize and fact extract from history.
  # It then passes the pretext to the new prompt.
  # conversation-pretext-generator: "human-conversation"
  # Replace selected text
  filter: no
  # Keep stitching together until reaching this limit
  # This allows a full response for answers which may need n*max-tokens to reach the stop-sequence.
  stitch-max: 0

  n-test-runs: 5
  # Prompt function aliases
  # aliases:
  # - "asktutor"
  # postprocessor: "sed 's/- //' | uniqnosort"
  # # Run it n times and combine the output
  # n-collate: 10
  # This for combining prompts:
  # It might be, for example, summarize, or uniqnosort
  # collation-postprocessor: "uniqnosort"
  # examplary continuation function
  # prompts are not stitched together / composed but examplary functions are
  # <g> is the existing generation
  # <1> is a variable
  continuation-function: "list-of <1> <g>"
#+END_SRC

** Tooling
If you are looking for a tool which can load
and make use of these =.prompt= files
directly, you may use =pen.el=, a package of
emacs that was used to generate them.

https://github.com/semiosis/pen.el

** Notes
- Trailing whitespace is always removed from the prompt before it is sent to the LM.